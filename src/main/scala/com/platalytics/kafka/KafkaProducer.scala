package com.platalytics.kafka

import scala.collection.JavaConversions._
import joptsimple._
import java.util.{Properties, UUID}
import java.io._
import kafka.common._
import kafka.message._
import kafka.serializer._
import java.util.Properties
import kafka.producer.Producer
import kafka.producer.ProducerConfig
import kafka.producer.KeyedMessage

case class KafkaProducer(
  topic: String, 
  brokerList: String, 
  /** brokerList
  * This is for bootstrapping and the producer will only use it for getting metadata (topics, partitions and replicas). 
  * The socket connections for sending the actual data will be established based on the broker information returned in 
  * the metadata. The format is host1:port1,host2:port2, and the list can be a subset of brokers or a VIP pointing to a 
  * subset of brokers.
  */
  //clientId: String = UUID.randomUUID().toString,
  /** clientId
  * The client id is a user-specified string sent in each request to help trace calls. It should logically identify 
  * the application making the request.
  */
  compress: Boolean = true,
  /** compress
  * This parameter allows you to specify the compression codec for all data generated by this producer. 
  * When set to true gzip is used.  To override and use snappy you need to implement that as the default
  * codec for compression using SnappyCompressionCodec.codec instead of DefaultCompressionCodec.codec below.
  */

  batchSize: Integer = 200,
  /** batchSize
  * The number of messages to send in one batch when using async mode. 
  * The producer will wait until either this number of messages are ready 
  * to send or queue.buffer.max.ms is reached.
  */
  messageSendMaxRetries: Integer = 3
  /** messageSendMaxRetries
  * This property will cause the producer to automatically retry a failed send request. 
  * This property specifies the number of retries when such failures occur. Note that 
  * setting a non-zero value here can lead to duplicates in the case of network errors 
  * that cause a message to be sent but the acknowledgement to be lost.
  */
  ) { 

  val props = new Properties()

  val codec = if(compress) DefaultCompressionCodec.codec else NoCompressionCodec.codec

  props.put("compression.codec", codec.toString)
  props.put("producer.type", "sync")
  props.put("metadata.broker.list", brokerList)
  props.put("batch.num.messages", batchSize.toString)
  props.put("message.send.max.retries", messageSendMaxRetries.toString)
  props.put("request.required.acks","-1")
  //props.put("client.id",clientId.toString)

  val producer = new Producer[AnyRef, AnyRef](new ProducerConfig(props))
  
  def kafkaMesssage(message: Array[Byte], partition: Array[Byte]): KeyedMessage[AnyRef, AnyRef] = {
     if (partition == null) {
       new KeyedMessage(topic,message)
     } else {
       new KeyedMessage(topic,partition,message)
     }
  }
  
  def send(message: String, partition: String = null): Unit = send(message.getBytes("UTF8"), if (partition == null) null else partition.getBytes("UTF8"))

  def send(message: Array[Byte], partition: Array[Byte]): Unit = {
    try {
      producer.send(kafkaMesssage(message, partition))
    } catch {
      case e: Exception =>
        e.printStackTrace
        System.exit(1)
    }        
  }
}